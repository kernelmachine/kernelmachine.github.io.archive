<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      Autodifferentiation and Backpropagation &middot; suchin
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/styles.css">
  <!-- <link rel="stylesheet" href="/googlecode.css"> -->

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/square-outline.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="suchin" href="/atom.xml">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75615851-1', 'auto');
    ga('send', 'pageview');

  </script>

</head>


  <body>

    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home" style="text-decoration: none;">suchin</a>
          <small><a href="/" style="text-decoration: none; color: #C0C0C0">writings</a></small>
          <div class="links">
          <a  href="http://github.com/pegasos1"> <img style="display: inline" src="https://assets-cdn.github.com/images/modules/logos_page/GitHub-Mark.png"   href="http://github.com/pegasos1" height="50px" width="50px"></img></a>
          <a  href="http://angel.co/suchin-gururangan"> <img style="display: inline" src="https://angel.co/images/shared/peace_large.jpg"   href="http://angel.co/ssgrn" height="50px" width="50px"></img></a>
          <a  href="http://linkedin.com/in/ssgrn" > <img style="display: inline" src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/LinkedIn_logo_initials.png/768px-LinkedIn_logo_initials.png"   height="50px" width="50px"></img></a>
          <a  href="http://twitter.com/ssgrn" > <img style="display: inline; margin-left: 15px;" src="https://upload.wikimedia.org/wikipedia/de/thumb/9/9f/Twitter_bird_logo_2012.svg/1000px-Twitter_bird_logo_2012.svg.png"   height="50px" width="50px"></img></a>
        </div>
        </h3>
      </header>

      <main>
        <article class="post">
  <h1 class="post-title">Autodifferentiation and Backpropagation</h1>
  <time datetime="2017-03-18T00:00:00-07:00" class="post-date">18 Mar 2017</time>
  <p>The derivative is an important operation in machine learning, primarily in the context of numerical optimization on loss functions.
However, since the derivative is an analytic procedure in the continuous realm, its translation to discrete computation has required significant work. This post will detail the various approaches that we can take to compute the derivative of arbitrary functions. First, we’ll discuss numerical differentiation, which while simple and intuitive, suffer from floating point rounding errors. Next, we’ll discuss symbolic differentiation, which suffers from complexity problems. Last, we’ll discuss auto-differentiation, the method of choice to compute derivatives of arbitrary functions with both exactness and simplicity. We’ll also discuss <em>backpropagation</em>, the method of updating weights in neural networks during learning that is an analogue to auto-differentiation.</p>

<h2 id="numerical-differentiation">Numerical differentiation</h2>

<p>Numerical differentation is rooted in the <em>finite differences approximation</em> definition of the derivative. Formally, for a function <script type="math/tex">f : \mathbb{R}^N \rightarrow \mathbb{R}</script>, we can compute the gradient <script type="math/tex">\triangledown f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n})</script> as</p>

<script type="math/tex; mode=display">\frac{\partial f(x)}{\partial x_i} \approx \frac{f(x_i + h) - f(x_i)}{h}</script>

<p>where <script type="math/tex">h > 0</script> is some small step size.</p>

<p>The issue with numerical differentiation is that it is inherently ill-conditioned and unstable. Essentially, we have to squeeze the infinite representation of real numbers into a finite representation of (usually) 32 or 64 bits. Many real number computations, such as the numerical derivative, cannot be exactly fitted to a finite representation without rounding or truncation, thus introducing approximation errors in the computation.</p>

<h2 id="symbolic-differentiation">Symbolic differentiation</h2>

<p>Another popular method of differentiation employed by services like Mathematica is <em>symbolic differentiation</em>, the automatic algebraic manipulation of expressions based on well known rules of differentation such as:</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial x}(f(x) + g(x)) = \frac{\partial f}{\partial x} + \frac{\partial g}{\partial x}</script>

<p>or</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial x}(\frac{f(x)}{g(x)}) = \frac{g(x)\frac{\partial f}{\partial x} - f(x)\frac{\partial g}{\partial x}}{g(x)^2}</script>

<p>In symbolic differentiation, expressions are sometimes represented as a trees of symbols that get manipulated by these basic rules to build an exact formula for computation. However, these trees can quickly become complex, leading to a ton of inefficiency.</p>

<h2 id="forward-autodifferentiation">Forward Autodifferentiation</h2>

<p>Autodifferentiation (AD) is the method of differentiation that is both exact and simple. The basic motivation for AD is that <em>any arbitrary function can be represented as a composition of simpler ones</em>. Under functional composition, one can compute the derivative of a function with the <em>chain rule</em>:</p>

<script type="math/tex; mode=display">f(x) = g(h(x)) \implies f'(x) = g'(h(x)) h'(x)</script>

<p>Because <script type="math/tex">g</script> and <script type="math/tex">h</script> are assumed to be simpler components of the arbitrary function <script type="math/tex">f</script>, their derivatives are also simpler to compute.</p>

<p>An example would help explain this idea.</p>

<p>Let</p>

<script type="math/tex; mode=display">f(x) = \frac{ln(x) + x^2} {sin(x)}</script>

<p>Taking the derivative of <script type="math/tex">f</script> analytically would be pretty annoying. Fortunately, <script type="math/tex">f</script> is a composition of very simple functions:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& v_0 = ln(x) \\
& v_1 = x^2 \\
& v_2 = v_0 + v_1 \\
& v_3 = sin(x)\\
& v_4 = \frac{v_2}{v_3}\\
& f = v_4
\end{align} %]]></script>

<p>Let <script type="math/tex">\dot v_i = \frac{\partial v_i}{\partial x}</script>. Differentiation proceeds via a forward pass:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& \dot v_0 = \frac{1}{x} \\
& \dot v_1 = 2x \\
& \dot v_2 = \dot v_0 + \dot v_1 = \frac{1}{x} + 2x\\
& \dot v_3 = cos(x)\\
& \dot v_4 = \frac{v_3 \dot v_2 - v_2 \dot v_3 }{v_3^2}\\
& \dot f = \dot v_4 = \frac{sin(x)(\frac{1}{x} + 2x) - (ln(x) + x^2)cos(x)}{sin^2(x)}
\end{align} %]]></script>

<p>The important things to notice are that:</p>

<ul>
  <li>
    <p>The forward computation of <script type="math/tex">\dot f</script> involves the <strong>staged computation</strong> of very simple components of the function. These stages are trivial to compute.</p>
  </li>
  <li>
    <p>There is a linear flow through the derivative computation, which lends itself well to imperative or iterative programming.</p>
  </li>
  <li>
    <p>We compute the derivative <em>exactly</em>, and do not approximate any value.</p>
  </li>
</ul>

<p>Now, let’s look at the multi-dimensional case. Let <script type="math/tex">f : \mathbb{R}^N \rightarrow \mathbb{R}^M</script>.</p>

<p>Then the derivative of <script type="math/tex">f</script> is expressed by the Jacobian matrix</p>

<script type="math/tex; mode=display">% <![CDATA[
J_f =

\begin{bmatrix}
\partial f_1 \over \partial x_1 & ... & \partial f_1 \over x_m \\
\vdots  & \ddots & \vdots \\
\partial f_n \over \partial x_1 &  ... & \partial f_n \over \partial x_m \\
\end{bmatrix} %]]></script>

<p><script type="math/tex">J_f</script> can be computed in just <script type="math/tex">n</script> forward AD passes across each dimension of <script type="math/tex">f</script>.</p>

<p>When <script type="math/tex">% <![CDATA[
N << M %]]></script>, then forward pass is extremely efficient in computing the Jacobian.</p>

<p>However, when <script type="math/tex">N >> M</script> another version of AD, called <em>reverse AD</em>, is more efficent to compute derivatives of <script type="math/tex">f</script> with respect to each input dimension. We’ll dig into that next.</p>

<h2 id="reverse-autodifferentiation">Reverse Autodifferentiation</h2>

<p>As opposed to forward autodifferentiation, which involves staged computations from the input to the output, reverse autodifferentiation evolves backwards from the function output.</p>

<p>We propagate the derivative of <script type="math/tex">f</script> with the <em>adjoint</em> operator</p>

<script type="math/tex; mode=display">\bar v_i = \frac{\partial f}{\partial v_i}</script>

<p>where <script type="math/tex">v_i</script> is some intermediate stage in the overall function computation.</p>

<p>Reverse AD proceeds in two phases, one of a forward pass computation, and then reverse accumulation. The forward pass, like we outlined in the previous section, helps us keep track of the computational stages that make up the arbitrary function <script type="math/tex">f</script>. The reverse accumulation then measures the sensitivity of the output of the forward pass with respect to a particular stage (ie, <script type="math/tex">\bar v_i</script>). We continue to compute derivatives backwards until we arrive at the original input: <script type="math/tex">\bar x = \frac{\partial f}{\partial x}</script>. Reverse AD is the preferred procedure over Forward AD when the function <script type="math/tex">f : \mathbb{R}^N \rightarrow \mathbb{R}^M</script> we’re trying to find the derivative of has <script type="math/tex">N >> M</script> because in reverse AD we only have to make <script type="math/tex">m</script> passes to compute the multi-dimensional gradient <script type="math/tex">f</script>.
&amp; \bar v_3 = \bar v_4 \frac{\partial v_4}{\partial v_3} = \bar v_4 \textbf{1} <br />
&amp; \bar v_2 = \bar v_4 \frac{\partial v_4}{\partial v_2} = \bar v_4 \textbf{1}<br />
&amp; \bar v_1 = \bar v_3 \frac{\partial v_3}{\partial v_1} = \bar v_3 2v_1<br />
&amp; \bar v_0 = \bar v_2 \frac{\partial v_2}{\partial v_0} = \bar v_2 \frac{1}{v_0}<br />
&amp; \bar x_2 = \bar v_1<br />
&amp; \bar x_1 = \bar v_0<br />
\end{align}
$$</p>

<p>To compute <script type="math/tex">\bar x_1</script> and <script type="math/tex">\bar x_2</script>, we must recognize that <script type="math/tex">x_1</script> and <script type="math/tex">x_2</script> affect the output <script type="math/tex">f</script> in distinct ways.</p>

<p>In fact, it is helpful to view the forward pass as a computation graph, to visualize this point:</p>

<p><script type="math/tex">x_1</script> only affects <script type="math/tex">f</script> through <script type="math/tex">v_2</script> and <script type="math/tex">v_5</script>, which means that, through the multi-dimensional chain rule:</p>

<script type="math/tex; mode=display">\frac{\partial f}{\partial x_1} = \frac{\partial f}{\partial v_0} = \frac{\partial f}{\partial v_2}\frac{\partial v_2}{\partial v_0} + \frac{\partial f}{\partial v_5}\frac{\partial v_5 }{\partial v_0}</script>

<p>or</p>

<script type="math/tex; mode=display">\bar v_0 = \bar v_2 \frac{\partial v_2}{\partial v_0} +  \bar v_5 \frac{\partial v_5 }{\partial v_0}</script>

<p>On the other hand, <script type="math/tex">x_2</script> only affects <script type="math/tex">f</script> through <script type="math/tex">v_3</script> and <script type="math/tex">v_5</script>, so:</p>

<script type="math/tex; mode=display">\frac{\partial f}{\partial x_2} = \frac{\partial f}{\partial v_1} = \frac{\partial f}{\partial v_3}\frac{\partial v_3}{\partial v_1} + \frac{\partial f}{\partial v_5}\frac{\partial v_5 }{\partial v_1}</script>

<p>or</p>

<script type="math/tex; mode=display">\bar v_1 = \bar v_3 \frac{\partial v_3}{\partial v_1} +  \bar v_5 \frac{\partial v_5 }{\partial v_1}</script>

<p>Thus we show that with reverse AD, we can compute <script type="math/tex">\frac{\partial f}{\partial x_1}</script> and <script type="math/tex">\frac{\partial f}{\partial x_2}</script> with very elementary operations.</p>

<p>Reverse autodifferentation is known as <em>backpropagation</em> in deep learning, and forms the basic way that we update parameters of a neural network during learning. In the next section, we’ll dive into how to apply reverse autodifferentation train neural networks.</p>

<h2 id="backpropagation-in-deep-learning">Backpropagation in Deep Learning</h2>

<p>Consider a two layer neural network. Given an input matrix <script type="math/tex">X \in R^{N x M}</script> and output labels <script type="math/tex">y \in R^N</script>, let <script type="math/tex">W_1</script> and <script type="math/tex">W_2</script> be weights corresponding to layer 1 and 2 respectively, and <script type="math/tex">b_1</script> and <script type="math/tex">b_2</script> be bias vectors corresponding to each weight matrix. Between layer 1 and 2 imagine we have a leaky ReLU activation function <script type="math/tex">f(x) = max(0, x)</script>, and imagine that we apply a softmax classifier after layer 2 to squash its output between <script type="math/tex">[0, 1]</script>.</p>

<p>The neural network looks like the following:</p>

<p>We can model the neural network as a forward pass of staged computations from the input <script type="math/tex">X</script> to the softmax loss function:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& Y_1 = W_1X + b_1 \\
& A_1 = max(0, Y_1) \\
& Y_2 = W_2A_1 + b_2 \\
& L = SoftmaxLoss(Y_2)\\
\end{align} %]]></script>

<p>How do we define the <script type="math/tex">SoftmaxLoss</script> function?</p>

<p>Well, the softmax function applied to a score vector <script type="math/tex">x</script> is</p>

<script type="math/tex; mode=display">p_k = \frac{e^{x_k}}{\sum_j e^{x_j}}</script>

<p>The data loss function for a softmax classifier is</p>

<script type="math/tex; mode=display">L_i = -log(p_{y_i})</script>

<p>The softmax classifier loss for this network is defined as</p>

<script type="math/tex; mode=display">L = \underbrace{\frac{1}{N}\sum_i L_i}_\text{data loss} + \underbrace{\lambda [\sum_i\sum_j W_1^2 + \sum_i\sum_j W_2^2]}_\text{regularization loss}</script>

<p>During learning, we use gradient descent to optimize the network’s weight matrices <script type="math/tex">W_1</script> and <script type="math/tex">W_2</script>. We update the weights with their gradients on <script type="math/tex">L</script> , <script type="math/tex">\partial L \over \partial W_1</script> and <script type="math/tex">\partial L \over \partial W_2</script>.</p>

<p>To find <script type="math/tex">\partial L \over \partial W_1</script> and <script type="math/tex">\partial L \over \partial W_2</script>, we use backpropagation (or reverse AD).</p>

<p>We’ve already done the first phase of backpropagation above, the forward pass. Now we compute the backward pass.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& \bar Y_2 = \bar L \frac {\partial L}{\partial Y_2}\\
& \bar W_2 = \bar Y_2 \frac{\partial Y_2}{\partial W_2} = \bar Y_2 A_1 \\
& \bar b_2 = \bar Y_2 \frac{\partial Y_2}{\partial b_2} = \bar Y_2 \textbf{1} \\
& \bar A_1 = \bar Y_2 \frac{\partial Y_2}{\partial A_1} = \bar Y_2 W_2 \\
& \bar Y_1 = \bar A_1 \frac{\partial A_1}{\partial Y_1} \\
& \bar W_1 = \bar Y_1 \frac{\partial Y_1}{\partial W_1} = \bar Y_1 X \\
& \bar b_1 = \bar Y_1 \frac{\partial Y_1}{\partial b_1} = \bar Y_1 \textbf{1}
\end{align} %]]></script>

<p>So our task is to find <script type="math/tex">\bar W_1</script> and <script type="math/tex">\bar W_2</script>.</p>

<p>Let’s start with <script type="math/tex">\bar W_2</script>.</p>

<script type="math/tex; mode=display">\bar W_2 = \bar Y_2 A_1 = \bar L \frac {\partial L}{\partial Y_2} A_1 = \frac {\partial L}{\partial Y_2} A_1</script>

<p>To begin the backpropagation, we first have to find <script type="math/tex">\frac{\partial L}{\partial Y_2}</script>. <script type="math/tex">Y_2</script> is just an unnormalized score vector. We will set an alias <script type="math/tex">f  = Y_2</script>. Consider first the data loss, <script type="math/tex">H_j = \frac{1}{N}\sum_i L_i = -\frac{1}{N}\sum_i log(p_j)</script>.</p>

<p>When <script type="math/tex">j = y_i</script>,</p>

<script type="math/tex; mode=display">\frac{\partial H}{\partial f_{y_i}} = -\frac{1}{N} \sum_i log(p_{y_i}) \frac{\partial p_{y_i}}{\partial f_{y_i}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}}  \frac{\sum_j e^{f_{j}} e^{f_{y_i}} - e^{f_{y_i}} e^{f_{y_i}} } {\sum_j e^{f_j}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}} \frac{e^{f_{y_i}} (\sum_j e^{f_{j}}  - e^{f_{y_i}})}{\sum_j e^{f_j}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}} p_{y_i}(1 - p_{y_i})</script>

<script type="math/tex; mode=display">= p_{y_i} - 1</script>

<p>When <script type="math/tex">j \neq y_i</script>,</p>

<script type="math/tex; mode=display">\frac{\partial H}{\partial f_{j}} = -\frac{1}{N} \sum_i log(p_{y_i}) \frac{\partial p_{y_i}}{\partial f_{j}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}} \frac{0 - e^{f_{y_i}}e^{f_{j}}}{\sum_j e^{f_j}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}} (- p_{y_i}p_j)</script>

<script type="math/tex; mode=display">= p_j</script>

<p>So this means:</p>

<script type="math/tex; mode=display">% <![CDATA[
\triangledown H_j =
\begin{cases}
  p_j & \text{if } j\neq y_i \\    p_{j} - 1  & \text{if } j=y_i\
\end{cases} %]]></script>

<p>In other words,</p>

<script type="math/tex; mode=display">\triangledown H_k =  p_k - \mathbb{1}(k = y_i)</script>

<p>Where 1 is just a binary indicator function that evaluates to 1 when the predicate is true.</p>

<p>Taking into account the gradient of the regularization term in <script type="math/tex">L</script>,</p>

<script type="math/tex; mode=display">\triangledown L = \frac{\triangledown H}{N} + \lambda\sum\limits_{i}\sum\limits_{j}W_{ij}</script>

<p>Basically to get the derivative of <script type="math/tex">L</script>, we just subtract its output with a binary indicator that is set to 1 when the index is the one of the true class.</p>

<h2 id="code-em-up">Code ‘em up</h2>

<p>As mentioned before, autodifferentiation lends itself very well to iterative programming. Here we’ll create a neural network in Python and update its parameters during learning with backpropagation.</p>

<p>First, we perform the forward pass, computing the class scores for the input.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span>  <span class="kn">as</span> <span class="nn">np</span>

<span class="n">Y1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span> <span class="c"># first layer</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Y1</span><span class="p">)</span> <span class="c"># leaky ReLU</span>
<span class="n">Y2</span> <span class="o">=</span> <span class="n">A1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span><span class="o">+</span> <span class="n">b2</span> <span class="c"># second layer</span>
<span class="n">exp_Y2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Y2</span><span class="p">)</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">exp_Y2</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">exp_Y2</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c"># softmax</span>
</code></pre>
</div>

<p>Then, we compute the loss:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span>  <span class="kn">as</span> <span class="nn">np</span>

<span class="n">data_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">prob</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="p">])</span>
<span class="n">reg_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W1</span> <span class="o">*</span> <span class="n">W1</span><span class="p">)</span>  <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W2</span> <span class="o">*</span> <span class="n">W2</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">data_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="n">reg_loss</span>
</code></pre>
</div>

<p>Next we compute the backpropagation, computing the derivatives of the weights and biases.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span>  <span class="kn">as</span> <span class="nn">np</span>

<span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>

<span class="n">dscores</span> <span class="o">=</span> <span class="n">scores</span>
<span class="n">dscores</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
<span class="n">dscores</span> <span class="o">/=</span> <span class="n">N</span>

<span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dscores</span><span class="p">)</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">dhidden</span> <span class="o">=</span> <span class="n">dscores</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">dhidden</span><span class="p">[</span><span class="n">a1</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dhidden</span><span class="p">)</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dhidden</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W2</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W1</span>
</code></pre>
</div>

</article>
<!--
<aside class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2017/03/04/The-Support-Vector-Machine/">
            Crash Course on Support Vector Machines
            <small><time datetime="2017-03-04T00:00:00-08:00">04 Mar 2017</time></small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/12/28/Introducing-Utah/">
            Introducing Utah
            <small><time datetime="2016-12-28T00:00:00-08:00">28 Dec 2016</time></small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/09/13/The-PlayRust-Classifier/">
            The PlayRust Classifier
            <small><time datetime="2016-09-13T00:00:00-07:00">13 Sep 2016</time></small>
          </a>
        </h3>
      </li>
    
  </ul>
</aside> -->

      </main>

      <footer class="footer">
        <small>
          &copy; <time datetime="2017-03-18T22:50:47-07:00">2017</time>. All rights reserved. Built with Jekyll and Mathjax.
        </small>
      </footer>
    </div>

  </body>
</html>
