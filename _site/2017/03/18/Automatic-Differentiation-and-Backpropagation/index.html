<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      Automatic differentiation and Backpropagation &middot; suchin
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/styles.css">
  <!-- <link rel="stylesheet" href="/googlecode.css"> -->

  <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/square-outline.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="suchin" href="/atom.xml">
  <script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

    ga('create', 'UA-75615851-1', 'auto');
    ga('send', 'pageview');

  </script>

</head>


  <body>

    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home" style="text-decoration: none;">suchin</a>
          <small><a href="/" style="text-decoration: none; color: #C0C0C0">writings</a></small>
          <div class="links">
          <a  href="http://github.com/pegasos1"> <img style="display: inline" src="https://assets-cdn.github.com/images/modules/logos_page/GitHub-Mark.png"   href="http://github.com/pegasos1" height="50px" width="50px"></img></a>
          <a  href="http://angel.co/suchin-gururangan"> <img style="display: inline" src="https://angel.co/images/shared/peace_large.jpg"   href="http://angel.co/ssgrn" height="50px" width="50px"></img></a>
          <a  href="http://linkedin.com/in/ssgrn" > <img style="display: inline" src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/ca/LinkedIn_logo_initials.png/768px-LinkedIn_logo_initials.png"   height="50px" width="50px"></img></a>
          <a  href="http://twitter.com/ssgrn" > <img style="display: inline; margin-left: 15px;" src="https://upload.wikimedia.org/wikipedia/de/thumb/9/9f/Twitter_bird_logo_2012.svg/1000px-Twitter_bird_logo_2012.svg.png"   height="50px" width="50px"></img></a>
        </div>
        </h3>
      </header>

      <main>
        <article class="post">
  <h1 class="post-title">Automatic differentiation and Backpropagation</h1>
  <time datetime="2017-03-18T00:00:00-07:00" class="post-date">18 Mar 2017</time>
  <p><em>Much of this blog post was inspired by <a href="http://cs231n.github.io">CS231n</a> and <a href="https://arxiv.org/abs/1502.05767">this</a> paper. Highly recommended reads.</em></p>

<p>The derivative is an important operation in machine learning, primarily for parameter optimization with loss functions. However, computing analytic derivatives in the finite context of 32 or 64 bits has required significant work. This post will detail the various approaches that we can take to compute the derivative of arbitrary functions. First, we’ll discuss numerical differentiation, which, while simple and intuitive, suffers from floating point errors. Then, we’ll discuss symbolic differentiation, which suffers from complexity problems. Finally, we’ll discuss auto-differentiation, the most popular method to compute derivatives with both exactness and simplicity. We’ll also discuss <em>backpropagation</em>, an analogue of auto-differentiation that is the primary method of learning in neural networks.</p>

<h2 id="numerical-differentiation">Numerical differentiation</h2>

<p>Numerical differentation is rooted in the <em>finite differences approximation</em> definition of the derivative. Formally, for a function <script type="math/tex">f : \mathbb{R}^N \rightarrow \mathbb{R}</script>, we can compute the gradient <script type="math/tex">\triangledown f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n})</script> as</p>

<script type="math/tex; mode=display">\frac{\partial f}{\partial x_i} \approx \lim_{h \rightarrow 0} \frac{f(x_i + h) - f(x_i)}{h}</script>

<p>where <script type="math/tex">h > 0</script> is some small step size. Essentially, given some tangent line between <script type="math/tex">f(x_i)</script> and <script type="math/tex">f(x_i + h)</script>, we estimate its slope by calculating the slope of <em>secant line</em>  between <script type="math/tex">(x, f(x))</script> and <script type="math/tex">(x_i + h , f(x_i + h))</script>, and say that as <script type="math/tex">h</script> approaches 0, the derivative of the secant line approaches the derivative of the tangent line. Here’s a graphic of that process:</p>

<p><img src="https://pegasos1.github.io/public/20170318/num_diff.png" /></p>

<p>The issue with numerical differentiation is that it is inherently ill-conditioned and unstable. It cannot be exactly fitted to a finite representation without rounding or truncation, thus introducing approximation errors in the computation. The size of <script type="math/tex">h</script> directly correlates with the amount of instability in the differentiation. If <script type="math/tex">h</script> is too small, then the subtraction from <script type="math/tex">x_i</script> will yield a larger rounding error. On the other hand, if <script type="math/tex">h</script> is too large, the estimate of the slope of the tangent could be worse. Most people advise against using the finite differences approximation of the derivative in machine learning systems.</p>

<h2 id="symbolic-differentiation">Symbolic differentiation</h2>

<p>One insight is that if we were somehow able to algebraically map functions to their derivatives, then we could likely achieve near full precision on our derivatives. For example, it is well known that the derivative of <script type="math/tex">f(x) = x^2</script> is <script type="math/tex">2x</script>. Computing <script type="math/tex">2x</script> is trivial, compared to using the finite differences approximation, with all its computational problems.</p>

<p>This is one of the primary motives of <em>symbolic differentiation</em>, the automatic algebraic manipulation of expressions based on well known rules of differentation such as:</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial x}(f(x) + g(x)) = \frac{\partial f}{\partial x} + \frac{\partial g}{\partial x}</script>

<p>or</p>

<script type="math/tex; mode=display">\frac{\partial}{\partial x}(\frac{f(x)}{g(x)}) = \frac{g(x)\frac{\partial f}{\partial x} - f(x)\frac{\partial g}{\partial x}}{g(x)^2}</script>

<p>Symbolic differentiation is a representational problem, and can quickly become extremely complex, depending on the function to be manipulated. But once the representation is achieved, derivatives can be computed much more accurately than with finite differences approximation. I won’t be going into more detail on symbolic differentiation’s internals, mainly because it’s complex, but I’ll point you <a href="http://homepage.divms.uiowa.edu/~stroyan/CTLC3rdEd/3rdCTLCText/Chapters/ch6.pdf">here</a> if you want more details.</p>

<h2 id="automatic-differentiation-in-forward-mode">Automatic Differentiation in Forward Mode</h2>

<p>Automatic Differentiation (AD) is a method that is both exact and simple. The basic motivation for AD is that <em>any arbitrary function can be represented as a composition of simpler ones</em>. Under functional composition, one can compute the derivative of a function using the <em>chain rule</em>:</p>

<script type="math/tex; mode=display">f(x) = g(h(x)) \implies f'(x) = g'(h(x)) h'(x)</script>

<p>Because <script type="math/tex">g</script> and <script type="math/tex">h</script> are assumed to be simpler components of the arbitrary function <script type="math/tex">f</script>, their derivatives are also simpler to compute.</p>

<p>An example would help explain this idea.</p>

<p>Let</p>

<script type="math/tex; mode=display">f(x) = \frac{ln(x)(x+3) + x^2} {sin(x)}</script>

<p>We could take the derivative of <script type="math/tex">f</script> using the rules we learned in high school. But there is an easier way. Fortunately, <script type="math/tex">f</script> is a composition of very simple functions:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& v_0 = ln(x) \\
& v_1 = x + 3 \\
& v_2 = v_0v_1 \\
& v_3 = x^2 \\
& v_4 = v_2 + v_3 \\
& v_5 = sin(x)\\
& v_6 = \frac{v_4}{v_5}\\
& f = v_6
\end{align} %]]></script>

<p>Let <script type="math/tex">\dot v_i = \frac{\partial v_i}{\partial x}</script>. Differentiation proceeds via a forward pass:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& \dot v_0 = \frac{1}{x} \\
& \dot v_1 = 1\\
& \dot v_2 = v_0 \dot v_1 + \dot v_0 v_1 = ln(x) + \frac{1}{x}\\
& \dot v_3 = 2x \\
& \dot v_4 = \dot v_2 + \dot v_3 = ln(x) + \frac{1}{x} + 2x\\
& \dot v_5 = cos(x)\\
& \dot v_6 = \frac{v_5 \dot v_4 - v_4 \dot v_5 }{v_5^2}\\
& \dot f = \dot v_6 = \frac{sin(x)(ln(x) + \frac{1}{x} + 2x) - (ln(x)(x+3) + x^2)cos(x)}{sin^2(x)}
\end{align} %]]></script>

<p>The important things to notice are that:</p>

<ul>
  <li>
    <p>The forward computation of <script type="math/tex">\dot f</script> involves the <strong>staged computation</strong> of very simple components of the function. These stages are trivial to compute.</p>
  </li>
  <li>
    <p>There is a linear flow through the derivative computation, mostly involving mere substitutions, which lends itself well to imperative programming.</p>
  </li>
  <li>
    <p>We compute the derivative <em>exactly</em>, and do not approximate any value.</p>
  </li>
</ul>

<p>Now, let’s look at the multi-dimensional case. Let <script type="math/tex">f : \mathbb{R}^N \rightarrow \mathbb{R}^M</script>.</p>

<p>The derivative of <script type="math/tex">f</script> is expressed by the Jacobian matrix</p>

<script type="math/tex; mode=display">% <![CDATA[
J_f =

\begin{bmatrix}
\partial f_1 \over \partial x_1 & ... & \partial f_1 \over x_m \\
\vdots  & \ddots & \vdots \\
\partial f_n \over \partial x_1 &  ... & \partial f_n \over \partial x_m \\
\end{bmatrix} %]]></script>

<p><script type="math/tex">J_f</script> can be computed in just <script type="math/tex">n</script> forward AD passes across each dimension of <script type="math/tex">f</script>.</p>

<p>When <script type="math/tex">% <![CDATA[
N << M %]]></script>, then the forward pass is extremely efficient in computing the Jacobian.</p>

<p>However, when <script type="math/tex">N >> M</script>, another version of AD, called <em>reverse AD</em>, is more efficent to compute derivatives of <script type="math/tex">f</script> with respect to each input dimension. We’ll dig into that next.</p>

<h2 id="automatic-differentiation-in-reverse-mode">Automatic Differentiation in Reverse Mode</h2>

<p>As opposed to forward autodifferentiation, which involves staged computations from the input to the output, reverse autodifferentiation evolves backwards from the function output.</p>

<p>We propagate the derivative of <script type="math/tex">f</script> with the <em>adjoint</em> operator</p>

<script type="math/tex; mode=display">\bar v_i = \frac{\partial f}{\partial v_i}</script>

<p>where <script type="math/tex">v_i</script> is some intermediate stage in the overall function computation. These derivatives measure the sensitivity of the output of the forward pass with respect to a particular stage.</p>

<p>Reverse AD proceeds in two phases, one of a forward pass computation, and then reverse accumulation. The forward pass, like we outlined in the previous section, helps us keep track of the computational stages that make up the arbitrary function <script type="math/tex">f</script>.  We then compute derivatives backwards until we arrive at the original input: <script type="math/tex">\bar x = \frac{\partial f}{\partial x}</script>. Reverse AD is the preferred procedure when the function <script type="math/tex">f : \mathbb{R}^N \rightarrow \mathbb{R}^M</script> has <script type="math/tex">N >> M</script> because in reverse AD we only have to make <script type="math/tex">m</script> passes to compute the multi-dimensional gradient.</p>

<p>For example, if</p>

<script type="math/tex; mode=display">\triangledown f = (\frac{\partial f}{\partial x_1}, \frac{\partial f}{\partial x_2},...,\frac{\partial f}{\partial x_n})</script>

<p>Then we only have to do 1 pass of reverse AD to compute <script type="math/tex">\triangledown f</script>, while we have to do <script type="math/tex">N</script> passes of forward AD to get the same answer.</p>

<p>Let’s go through an example of doing reverse AD to make the procedure clearer.</p>

<p>Let</p>

<script type="math/tex; mode=display">f(x_1, x_2) = \frac{ln(x_1) + x_2^2} {sin(x_1)x_2}</script>

<p>First we do a forward pass on <script type="math/tex">f</script></p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& v_0 = x_1\\
& v_1 = x_2\\
& v_2 = ln(v_0) \\
& v_3 = v_1^2 \\
& v_4 = v_2 + v_3 \\
& v_5 = sin(v_0)v_1\\
& v_6 = \frac{v_4}{v_5}\\
& f = v_6
\end{align} %]]></script>

<p>To compute <script type="math/tex">\bar x_1 = \frac{\partial f}{\partial x_1}</script> and <script type="math/tex">\bar x_2 = \frac{\partial f}{\partial x_2}</script>, recognize that <script type="math/tex">x_1</script> and <script type="math/tex">x_2</script> affect the output <script type="math/tex">f</script> in distinct ways.</p>

<p>In fact, it is helpful to view the forward pass as a computation graph, to visualize this point:</p>

<p><img src="https://pegasos1.github.io/public/20170318/graph.png" /></p>

<p><script type="math/tex">x_1</script> only affects <script type="math/tex">f</script> through <script type="math/tex">v_2</script> and <script type="math/tex">v_5</script>, which means that through the multi-dimensional chain rule:</p>

<script type="math/tex; mode=display">\frac{\partial f}{\partial x_1} = \frac{\partial f}{\partial v_0} = \frac{\partial f}{\partial v_2}\frac{\partial v_2}{\partial v_0} + \frac{\partial f}{\partial v_5}\frac{\partial v_5 }{\partial v_0}</script>

<p>or</p>

<script type="math/tex; mode=display">\bar v_0 = \bar v_2 \frac{\partial v_2}{\partial v_0} +  \bar v_5 \frac{\partial v_5 }{\partial v_0}</script>

<p>On the other hand, <script type="math/tex">x_2</script> only affects <script type="math/tex">f</script> through <script type="math/tex">v_3</script> and <script type="math/tex">v_5</script>, so:</p>

<script type="math/tex; mode=display">\frac{\partial f}{\partial x_2} = \frac{\partial f}{\partial v_1} = \frac{\partial f}{\partial v_3}\frac{\partial v_3}{\partial v_1} + \frac{\partial f}{\partial v_5}\frac{\partial v_5 }{\partial v_1}</script>

<p>or</p>

<script type="math/tex; mode=display">\bar v_1 = \bar v_3 \frac{\partial v_3}{\partial v_1} +  \bar v_5 \frac{\partial v_5 }{\partial v_1}</script>

<p>So, we can compute <script type="math/tex">\frac{\partial f}{\partial x_1}</script> and <script type="math/tex">\frac{\partial f}{\partial x_2}</script> with very elementary operations.</p>

<p>To calculate these decompositions of our gradient, we begin at the output <script type="math/tex">f = v_6</script> and propagate its derivative backwards:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& \bar v_6 = \bar f\\
& \bar v_5 = \bar v_6 \frac{\partial v_6}{\partial v_5} = \bar v_6 \frac{-v_4}{v_5^2}\\
& \bar v_4 = \bar v_6 \frac{\partial v_6}{\partial v_4} = \bar v_6 \frac{1}{v_5}  \\
& \bar v_3 = \bar v_4 \frac{\partial v_4}{\partial v_3} = \bar v_4 \textbf{1} \\
& \bar v_2 = \bar v_4 \frac{\partial v_4}{\partial v_2} = \bar v_4 \textbf{1}\\
& \bar v_1 = \bar v_3 \frac{\partial v_3}{\partial v_1} = \bar v_3 2v_1\\
& \bar v_0 = \bar v_2 \frac{\partial v_2}{\partial v_0} = \bar v_2 \frac{1}{v_0}\\
& \bar x_2 = \bar v_1\\
& \bar x_1 = \bar v_0\\
\end{align} %]]></script>

<p>By keeping track of stages in the forward pass of the reverse AD, the bottleneck of computing <script type="math/tex">\frac{\partial f}{\partial x_1}</script> and  <script type="math/tex">\frac{\partial f}{\partial x_2}</script> is reduced to computing <script type="math/tex">\bar v_6</script>, which is only dependent on the complexity of the final stage of computation in <script type="math/tex">f</script>. As an exercise, set <script type="math/tex">\bar v_6 = 1</script> and compute the gradient of <script type="math/tex">f</script>.</p>

<p>Reverse autodifferentation is known as <em>backpropagation</em> in deep learning, and forms the basic way that we update parameters of a neural network during learning. In the next section, we’ll dive into how to apply reverse autodifferentation to train neural networks.</p>

<h2 id="backpropagation-in-deep-learning">Backpropagation in Deep Learning</h2>

<p>Consider a two layer neural network. Given an input matrix <script type="math/tex">X \in R^{N x M}</script> and output labels <script type="math/tex">y \in R^N</script>, let <script type="math/tex">W_1</script> and <script type="math/tex">W_2</script> be weight matrices corresponding to layer 1 and 2 respectively, and <script type="math/tex">b_1</script> and <script type="math/tex">b_2</script> be bias vectors corresponding to each weight matrix. Between layer 1 and 2 imagine we have an ReLU activation function <script type="math/tex">f(x) = max(0, x)</script>, and imagine that we apply a softmax classifier after layer 2 to squash its output between <script type="math/tex">[0, 1]</script>.</p>

<p>Here’s a rough diagram of the network we’ll be working with.</p>

<p><img src="https://pegasos1.github.io/public/20170318/nnet.png" /></p>

<p>During learning, we want to provide input to the neural network, and then update the weights at each layer depending on the error computed by the loss function at the output. We’ll use reverse AD (or <em>backpropagation</em>) to find the gradients of the loss function with respect to each weight matrix. Note that all the layers can be updated by merely knowing the derivative of the <em>last stage of computation</em>, because as we saw in the last section, all previous stages’ derivatives can then be computed. This means that we’ll have to figure out what the derivative of our the softmax loss function is, and then we’re golden.</p>

<p>We can model the neural network as a forward pass of staged computations from the input <script type="math/tex">X</script> to the softmax loss function:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& Y_1 = W_1X + b_1 \\
& A_1 = max(0, Y_1) \\
& Y_2 = W_2A_1 + b_2 \\
& L = SoftmaxLoss(Y_2)\\
\end{align} %]]></script>

<p>Here’s what the forward pass (prior to the loss function) would look like in Python, computing the class scores for the input.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span>  <span class="kn">as</span> <span class="nn">np</span>

<span class="n">Y1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span> <span class="c"># first layer</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Y1</span><span class="p">)</span> <span class="c">#  ReLU activation</span>
<span class="n">Y2</span> <span class="o">=</span> <span class="n">A1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span><span class="o">+</span> <span class="n">b2</span> <span class="c"># second layer</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Y2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Y2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c"># softmax</span>
</code></pre>
</div>

<p>How do we define the <script type="math/tex">SoftmaxLoss</script> function in the output?</p>

<p>Well, the softmax function applied to a score vector <script type="math/tex">f</script> is</p>

<script type="math/tex; mode=display">p_k = \frac{e^{f_k}}{\sum_j e^{f_j}}</script>

<p>The data loss function for a softmax classifier is</p>

<script type="math/tex; mode=display">L_i = -log(p_{y_i})</script>

<p>Where <script type="math/tex">y_i</script> is the index of the correct label.</p>

<p>The softmax classifier loss for this network is defined as</p>

<script type="math/tex; mode=display">L = \underbrace{\frac{1}{N}\sum_i L_i}_\text{data loss} + \underbrace{\lambda [\sum_i\sum_j W_1^2 + \sum_i\sum_j W_2^2]}_\text{regularization loss}</script>

<p>Rounding out the code for the forward pass, here’s what the loss function looks like in Python:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span>  <span class="kn">as</span> <span class="nn">np</span>

<span class="n">data_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="p">])</span>
<span class="n">reg_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W1</span> <span class="o">*</span> <span class="n">W1</span><span class="p">)</span>  <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W2</span> <span class="o">*</span> <span class="n">W2</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">data_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="n">reg_loss</span>
</code></pre>
</div>

<p>During learning, we use gradient descent to optimize the network’s weight matrices <script type="math/tex">W_1</script> and <script type="math/tex">W_2</script>. We update the weights with their gradients on <script type="math/tex">L</script> , <script type="math/tex">\partial L \over \partial W_1</script> and <script type="math/tex">\partial L \over \partial W_2</script>.</p>

<p>To find <script type="math/tex">\partial L \over \partial W_1</script> and <script type="math/tex">\partial L \over \partial W_2</script>, we do the reverse accumulation phase of backpropagation.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
& \bar Y_2 = \bar L \frac {\partial L}{\partial Y_2}\\
& \bar W_2 = \bar Y_2 \frac{\partial Y_2}{\partial W_2} = \bar Y_2 A_1 \\
& \bar b_2 = \bar Y_2 \frac{\partial Y_2}{\partial b_2} = \bar Y_2 \textbf{1} \\
& \bar A_1 = \bar Y_2 \frac{\partial Y_2}{\partial A_1} = \bar Y_2 W_2 \\
& \bar Y_1 = \bar A_1 \frac{\partial A_1}{\partial Y_1} \\
& \bar W_1 = \bar Y_1 \frac{\partial Y_1}{\partial W_1} = \bar Y_1 X \\
& \bar b_1 = \bar Y_1 \frac{\partial Y_1}{\partial b_1} = \bar Y_1 \textbf{1}
\end{align} %]]></script>

<p>So our task is to find <script type="math/tex">\bar W_1</script> and <script type="math/tex">\bar W_2</script>.</p>

<p>Let’s start with <script type="math/tex">\bar W_2</script>.</p>

<script type="math/tex; mode=display">\bar W_2 = \bar Y_2 A_1 = \bar L \frac {\partial L}{\partial Y_2} A_1 = \frac {\partial L}{\partial Y_2} A_1</script>

<p>We want to find <script type="math/tex">\frac{\partial L}{\partial Y_2}</script>. <script type="math/tex">Y_2</script> is just an unnormalized score vector. In the following equations, we will set an alias <script type="math/tex">f  = Y_2</script> for clarity. Consider first the data loss, <script type="math/tex">H_j = \frac{1}{N}\sum_i L_i = -\frac{1}{N}\sum_i log(p_j)</script>.</p>

<p>When <script type="math/tex">j = y_i</script>,</p>

<script type="math/tex; mode=display">\frac{\partial H}{\partial f_{y_i}} = -\frac{1}{N} \sum_i \frac{\partial}{\partial f_{y_i}} log(p_{y_i}) \frac{\partial p_{y_i}}{\partial f_{y_i}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}}  \frac{\sum_j e^{f_{j}} e^{f_{y_i}} - e^{f_{y_i}} e^{f_{y_i}} } {\sum_j e^{f_j}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}} \frac{e^{f_{y_i}} (\sum_j e^{f_{j}}  - e^{f_{y_i}})}{\sum_j e^{f_j}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}} p_{y_i}(1 - p_{y_i})</script>

<script type="math/tex; mode=display">= p_{y_i} - 1</script>

<p>When <script type="math/tex">j \neq y_i</script>,</p>

<script type="math/tex; mode=display">\frac{\partial H}{\partial f_{j}} = -\frac{1}{N} \sum_i \frac{\partial}{\partial f_{j}} log(p_{y_i}) \frac{\partial p_{y_i}}{\partial f_{j}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}} \frac{0 - e^{f_{y_i}}e^{f_{j}}}{\sum_j e^{f_j}}</script>

<script type="math/tex; mode=display">= -\frac{1}{N} \sum_i \frac{1}{p_{y_i}} (- p_{y_i}p_j)</script>

<script type="math/tex; mode=display">= p_j</script>

<p>So this means:</p>

<script type="math/tex; mode=display">% <![CDATA[
\triangledown H_j =
\begin{cases}
  p_j & \text{if } j\neq y_i \\    p_{j} - 1  & \text{if } j=y_i\
\end{cases} %]]></script>

<p>In other words,</p>

<script type="math/tex; mode=display">\triangledown H_k =  p_k - \mathbb{1}(k = y_i)</script>

<p>Where <script type="math/tex">\mathbb{1}</script> is just a binary indicator function that evaluates to 1 when the predicate is true.</p>

<p>Now we can code the backpropagation onto the softmax loss function:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># backprop onto loss function</span>
<span class="n">dscores</span> <span class="o">=</span> <span class="n">scores</span>
<span class="n">dscores</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
<span class="n">dscores</span> <span class="o">/=</span> <span class="n">N</span>
</code></pre>
</div>

<p>Taking into account the gradient of the regularization term in <script type="math/tex">L</script>,</p>

<script type="math/tex; mode=display">\triangledown L = \frac{\triangledown H}{N} + \lambda\sum\limits_{i}\sum\limits_{j}W_{ij}</script>

<p>We’ll add the regularization gradient at the end of the backpropagation.</p>

<p>With the gradient of the loss function, we can easily calculate <script type="math/tex">\bar W_2</script> and <script type="math/tex">\bar b_2</script></p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>
<span class="c"># backprop onto W2 and b2</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dscores</span><span class="p">)</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre>
</div>

<p>Now we have to compute <script type="math/tex">\bar W_1</script>.</p>

<script type="math/tex; mode=display">\bar W_2 = \bar Y_1 X  = \bar A_1 \frac{\partial A_1}{\partial Y_1} X = \bar Y_2 W_2 \frac{\partial A_1}{\partial Y_1} X</script>

<p>To compute <script type="math/tex">\bar W_1</script>, we first have to backpropagate onto the ReLU activation function. In other words, we have to find <script type="math/tex">\frac{\partial A_1}{\partial Y_1}</script>:</p>

<script type="math/tex; mode=display">A_1 = max(0, Y_1) \implies \frac{\partial A_1}{\partial Y_1}=\mathbb{1}( Y_1 >0)</script>

<p>Combined with the chain rule, we see that the ReLU unit lets the gradient pass through unchanged if its input was greater than 0, but kills it if its input was less than zero during the forward pass.</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># backprop onto ReLU activation</span>
<span class="n">dhidden</span> <span class="o">=</span> <span class="n">dscores</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">dhidden</span><span class="p">[</span><span class="n">a1</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
</code></pre>
</div>

<p>Now we can calculate <script type="math/tex">\bar W_1</script> and <script type="math/tex">\bar b_1</script>:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># backprop onto W1 and b2</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dhidden</span><span class="p">)</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dhidden</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
</code></pre>
</div>

<p>Finally, remember to add in the backpropagation onto the regularization loss:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="c"># don't forget regularization loss</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W2</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W1</span>
</code></pre>
</div>

<p>Here’s the full code to compute the backpropagation on our network:</p>

<div class="highlighter-rouge"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">numpy</span>  <span class="kn">as</span> <span class="nn">np</span>

<span class="c">## forward pass</span>
<span class="n">Y1</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W1</span><span class="p">)</span> <span class="o">+</span> <span class="n">b1</span> <span class="c"># first layer</span>
<span class="n">A1</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">Y1</span><span class="p">)</span> <span class="c">#  ReLU activation</span>
<span class="n">Y2</span> <span class="o">=</span> <span class="n">A1</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="p">)</span><span class="o">+</span> <span class="n">b2</span> <span class="c"># second layer</span>
<span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Y2</span><span class="p">)</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">Y2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c"># softmax</span>

<span class="c">## loss function</span>
<span class="n">data_loss</span> <span class="o">=</span> <span class="o">-</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">scores</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span> <span class="n">y</span><span class="p">])</span>
<span class="n">reg_loss</span> <span class="o">=</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W1</span> <span class="o">*</span> <span class="n">W1</span><span class="p">)</span>  <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">W2</span> <span class="o">*</span> <span class="n">W2</span><span class="p">)</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">data_loss</span><span class="p">)</span> <span class="o">/</span> <span class="n">N</span> <span class="o">+</span> <span class="n">reg_loss</span>

<span class="n">grads</span> <span class="o">=</span> <span class="p">{}</span>

<span class="c"># backprop onto loss function</span>
<span class="n">dscores</span> <span class="o">=</span> <span class="n">scores</span>
<span class="n">dscores</span><span class="p">[</span><span class="nb">range</span><span class="p">(</span><span class="n">N</span><span class="p">),</span><span class="n">y</span><span class="p">]</span> <span class="o">-=</span> <span class="mi">1</span>
<span class="n">dscores</span> <span class="o">/=</span> <span class="n">N</span>

<span class="c"># backprop onto W2 and b2</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">a1</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dscores</span><span class="p">)</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'b2'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dscores</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c"># backprop onto ReLU activation</span>
<span class="n">dhidden</span> <span class="o">=</span> <span class="n">dscores</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">W2</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="n">dhidden</span><span class="p">[</span><span class="n">a1</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c"># backprop onto W1 and b2</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dhidden</span><span class="p">)</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'b1'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">dhidden</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>

<span class="c"># don't forget regularization loss</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W2'</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W2</span>
<span class="n">grads</span><span class="p">[</span><span class="s">'W1'</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reg</span> <span class="o">*</span> <span class="n">W1</span>
</code></pre>
</div>

<p>We would use these gradients to then update <script type="math/tex">W_1</script> and <script type="math/tex">W_2</script> with gradient descent.</p>

</article>
<!--
<aside class="related">
  <h2>Related Posts</h2>
  <ul class="related-posts">
    
      <li>
        <h3>
          <a href="/2017/03/04/The-Support-Vector-Machine/">
            Crash Course on Support Vector Machines
            <small><time datetime="2017-03-04T00:00:00-08:00">04 Mar 2017</time></small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/12/28/Introducing-Utah/">
            Introducing Utah
            <small><time datetime="2016-12-28T00:00:00-08:00">28 Dec 2016</time></small>
          </a>
        </h3>
      </li>
    
      <li>
        <h3>
          <a href="/2016/09/13/The-PlayRust-Classifier/">
            The PlayRust Classifier
            <small><time datetime="2016-09-13T00:00:00-07:00">13 Sep 2016</time></small>
          </a>
        </h3>
      </li>
    
  </ul>
</aside> -->

      </main>

      <footer class="footer">
        <small>
          &copy; <time datetime="2017-03-21T09:16:19-07:00">2017</time>. All rights reserved. Built with Jekyll and Mathjax.
        </small>
      </footer>
    </div>

  </body>
</html>
