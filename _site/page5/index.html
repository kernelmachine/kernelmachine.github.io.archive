<!DOCTYPE html>
<html lang="en">

  <head>
  <meta charset="UTF-8">
  <link href="http://gmpg.org/xfn/11" rel="profile">

  <!-- Enable responsiveness on mobile devices-->
  <meta name="viewport" content="width=device-width, initial-scale=1.0">

  <title>
    
      suchin &middot; writings
    
  </title>

  <!-- CSS -->
  <link rel="stylesheet" href="/styles.css">
  <script src="/highlight.pack.js"></script>
  <!-- <link rel="stylesheet" href="/googlecode.css"> -->

  <script>hljs.initHighlightingOnLoad();</script>
  <script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML" type="text/javascript"></script>

  <!-- Icons -->
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/public/apple-touch-icon-precomposed.png">
  <link rel="shortcut icon" href="/public/square-outline.ico">

  <!-- RSS -->
  <link rel="alternate" type="application/atom+xml" title="suchin" href="/atom.xml">
</head>


  <body>

    <div class="container content">
      <header class="masthead">
        <h3 class="masthead-title">
          <a href="/" title="Home" style="text-decoration: none;">suchin</a>
          <small><a href="/about" style="text-decoration: none; color: #C0C0C0">writings</a></small>
        </h3>
      </header>

      <main>
        <div class="posts">
  
  <article class="post">
    <h1 class="post-title">
      <a href="/2016/02/16/applying-machine-learning-to-security-problems/">
        Applying Machine Learning to Security Problems
      </a>
    </h1>

    <time datetime="2016-02-16T00:00:00-05:00" class="post-date">16 Feb 2016</time>

    <p>Anomaly detection and remediation is a hard and expensive process ridden with
false alarms and rabbit holes. The security community has been increasingly
interested in the potential for data-driven tools to filter out noise and automatically
detect malicious activity in large networks. However, while capable
of overcoming the limitations of static, rule-based techniques, machine learning
is not a silver bullet solution to detecting and responding to attacks. In this
post, we will detail two obstacles in applying machine learning to security, with
attention to how to solve them.</p>

<p>At its core, the (supervised) learning process is two-fold: training a model on
known data and testing it on unknown samples. In particular, adaptable models
require a continuous flow of labeled data to train with. Unfortunately, the
creation of such labeled data is the most expensive and time-consuming part
of the data science process. Data is usually messy, incomplete, and inconsistent.
While there are many tools to experiment with different algorithms and
their parameters, there are few tools to help one develop clean, comprehensive
datasets. Often times this means asking practitioners with deep domain expertise
to help label existing data elements. You can also try to purchase “good” data,
but this can be hard to come by in the security context (and may go stale very quickly).
You can also try to use a combination of unsupervised and supervised learning called—unsurprisingly—semi-supervised learning</p>

<p>On top of that, bias in training data can hamper the effectiveness of a model
to discern between output classes. In the security context, data bias can be
interpreted in two ways.</p>

<p>First, attack methodologies are becoming more dynamic than ever before. If a
predictive model is trained on known patterns and vulnerabilities, it may not
necessarily detect an unprecedented attack that does not conform to to those
1trends. If a predictive model is trained on known patterns and vulnerabilities
(i.e. using features from malware that is file-system resident),
it may not necessarily detect an unprecedented attack that does not conform
 to those trends (i.e. misses features from malware that is only memory resident).</p>

<p>Bias can sneak up on you, as well. You may think you can use the Alexa listings to,
say, obtain a list of benign domains, but that assumption may turn out to be a bad
idea since there is no guarantee that those sites are clean. Getting good ground
truth in security is hard.</p>

<p>Second, data bias also comes in the form of <em>class representation</em>. To understand
class representation bias, one can look to a core foundation of statistics: Bayes
theorem.</p>

<p>Bayes theorem describes the probability of event A given event B:</p>

<script type="math/tex; mode=display">P(A | B) = \frac { P (A) P(B | A)}{P(B)}</script>

<p>Expanding the probability P (B) for the set of two mutually exclusive outcomes,
we arrive at the following equation:</p>

<script type="math/tex; mode=display">P (B) = (A 1 )P (B|A 1 ) + (\neg A 2 )P (B|\neg A 2 )</script>

<p>Combining the above equations, we arrive at the following alternative statement
of Bayes’ theorem:</p>

<script type="math/tex; mode=display">P (A|B) = \frac{P (A)P (B|A)} {P (A 1 )P (B|A 1 ) + P (\neg A 2 )P (B|\neg A 2 )}</script>

<p>Let’s apply this theorem to a concrete security problem to show the emergent
issues of training predictive models on biased data.</p>

<p>Suppose company <script type="math/tex">X</script> has <script type="math/tex">1000</script> employees, and a security vendor has deployed an
intrusion detection system (IDS) alerting the company <script type="math/tex">X</script> when it detects a malicious
URL sent to an employee’s inbox. Suppose there are <script type="math/tex">10</script> malicious URLs
sent to employees of company <script type="math/tex">X</script> per day. Finally, suppose the IDS analyzes
<script type="math/tex">10000</script> incoming URLs to company <script type="math/tex">X</script> per day.</p>

<p>Let <script type="math/tex">I</script>denote an incident (an incoming malicious URL) and <script type="math/tex">\neg I</script> denote a non-
incident (an incoming benign URL). Similarly, let <script type="math/tex">A</script> denote an alarm (the
IDS classifies incoming URL as malicious) and <script type="math/tex">\neg A</script> denote a non-alarm (the
IDS classifies URL as benign). That means <script type="math/tex">P (A|I) = P (\text{hit})</script> and <script type="math/tex">P (A| \neg I) =
P (\text{false alarm})</script>.</p>

<p>What’s the probability that an alarm is associated with a real incident? In other
words, how much can we trust the IDS under these conditions?</p>

<p>Using Bayes’ Theorem from above, we know:</p>

<script type="math/tex; mode=display">P (I|A) = \frac{P (I)P (A|I)}{P (I)P (A|I) + P (\neg I)P (A|\neg I)}</script>

<p>Put another way,</p>

<script type="math/tex; mode=display">P (\text{IDS is accurate}) = \frac{P (\text{incident})P (\text{hit})}{P (\text{incident})P (\text{hit}) + P (\text{non-incident})P (\text{false alarm})}</script>

<p>Now let’s calculate <script type="math/tex">P(\text{incident})</script> and <script type="math/tex">P(\text{non-incident})</script>, given the parameters of
the IDS problem we defined above:</p>

<script type="math/tex; mode=display">P(\text{incident}) =\frac{\text{10 incidents per day}}{\text{10000 audits per day}}</script>

<script type="math/tex; mode=display">P (\text{non-incident}) = 1 − P (\text{incident}) = 0.999</script>

<p>These probabilities emphasize the bias present in the distribution of analyzed
URLs. The IDS has little sense of what incidents entail, as it is trained on very
few examples of it. Plugging the probabilities into the equation above, we find
that:</p>

<p>Thus, to have reasonable confidence in an IDS under these biased conditions,
we must have not only unrealistically high hit rate, but also unrealistically low
false positive rate. For example, for an IDS to be <script type="math/tex">80</script> percent accurate, even with
a best case scenario of a 100 percent hit rate, the IDS’ false alarm rate must be
<script type="math/tex">4 \times 10^{−4}</script> . In other words, only <script type="math/tex">4</script> out of <script type="math/tex">10000</script> alarms can be false
positives to achieve this accuracy.</p>

<p>In the real world, detection hit rates are much lower and false alarm rates are
much higher. Thus, class representation bias in the security context can make
machine learning algorithms inaccurate and untrustworthy. When models are
trained on only a few examples of one class but many examples of another, the
bar for reasonable accuracy is extremely high, and in some cases unachievable.
Predictive algorithms run the risk of being ”the boy who cried wolf” – annoying
and prone to desensitizing security professionals to incident alerts. That last thing
you want to do is create a fancy new system that only exacerbates the problem that
was identified at the core of the Target/Home Depot breaches.</p>

<p>Data scientists can avoid these obstacles with a few measures:</p>

<p>1) Data scientists should pay particular attention to training models with large
and balanced data that are representative of all output classes. Take balanced
subsamples of your data if necessary, and understand your data<sup id="fnref:1"><a href="#fn:1" class="footnote">1</a></sup>.</p>

<p>2) Data scientists should focus on getting a plethora of labeled data. Amazon’s
Mechanical Turk is a useful tool for this. Look at open sourced data, and encourage
data gathering expeditions.</p>

<p>3) Encourage security expertise on the team. Domain expertise is pretty important
to the performance of machine learning algorithms applied in the security
space. To keep up with the changing threat landscape, one must have security
experience.</p>

<p>4) Incorporate unsupervised methods into the solution of the data science problem.
Focus on organization, presentation, visualization, filtering of data - not
just prediction.</p>

<p>5) Weigh the trade-off between accuracy vs. coverage.</p>

<p>Machine learning has the potential to change how we detect and respond to
malicious activity in our networks. Machine learning can weed out signal from
noise to help incident responders focus on what’s important. It can help administrators
discover patterns in network activity never seen before. However, when applying these
algorithms to security we must be aware of caveats of the approach, so we may overcome
them</p>

<div class="footnotes">
  <ol>
    <li id="fn:1">
      <p><a href="http://divac.ist.temple.edu/~vucetic/documents/vucetic01ecml.pdf">Classification on Biased Data</a> <a href="#fnref:1" class="reversefootnote">&#8617;</a></p>
    </li>
  </ol>
</div>

  </article>
  
</div>

<div class="pagination">
  
    <a class="pagination-item older" href="/page6">Older</a>
  
  
    <a class="pagination-item newer" href="/page4">Newer</a>
  
</div>

      </main>

      <footer class="footer">
        <small>
          &copy; <time datetime="2016-02-27T14:49:00-05:00">2016</time>. All rights reserved. Built with Jekyll and Mathjax.
        </small>
      </footer>
    </div>

  </body>
</html>
